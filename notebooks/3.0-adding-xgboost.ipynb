{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c3c15e",
   "metadata": {},
   "source": [
    "# Store Item Demand Forecasting - Model Training\n",
    "This is the second notebook on this project where I'll be running a data modeling process on the product sales dataset available at the Kaggle's *Store Item Demand Forecasting Challende*.\n",
    "\n",
    "Here I'll be training Facebook's Prophet different models at scale, to cover all the products and stores.\n",
    "\n",
    "<img src=\"https://i.ibb.co/FDsQbZX/kaggle-comp-banner.png\" width=\"900\" />\n",
    "\n",
    "**Challenge Description**: <br>\n",
    "\n",
    "<i>\n",
    "This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.\n",
    "\n",
    "You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n",
    "\n",
    "What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?\n",
    "\n",
    "This is a great competition to explore different models and improve your skills in forecasting.\n",
    "</i>\n",
    "\n",
    "**Challenge Goal**: <br>\n",
    "\n",
    "<i>\n",
    "The objective of this competition is to predict 3 months of item-level sales data at different store locations. (with no holiday effect)\n",
    "</i>\n",
    "\n",
    "**Author**: Arthur G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bac52",
   "metadata": {},
   "source": [
    "## Loading Dependencies\n",
    "Here I'll be loading and setting up the dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5c5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# add libs\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet.serialize import model_to_json\n",
    "from src.visualization.visualize import model_performance_indicators\n",
    "\n",
    "# setting up libs\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('font', **{'family': 'DejaVu Sans', 'size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7817a",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Now it's time to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42726a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    913000 non-null  object\n",
      " 1   store   913000 non-null  int64 \n",
      " 2   item    913000 non-null  int64 \n",
      " 3   sales   913000 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sales_df = pd.read_csv(os.path.join('..', 'data', 'raw', 'train.csv'))\n",
    "sales_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27a504",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Here I'll be preparing the data for the modeling-at-scale process. Let's start by grouping at store_item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8579dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting column names to match Prophet requirements\n",
    "sales_df.rename(columns={'date': 'ds', 'sales': 'y'}, inplace=True)\n",
    "\n",
    "# grouping data\n",
    "modeling_groups = sales_df.groupby(['store', 'item'])\n",
    "modeling_groups_keys = list(modeling_groups.groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0b1d0",
   "metadata": {},
   "source": [
    "## Training with XGBoost\n",
    "This is a hybrid approach where I'm using XGBoost to improve the capacity of our model to \"understand\" non-stationary components.\n",
    "\n",
    "Let's start by training the prophet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd05249a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log joint probability = -16.7167\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99        4317.7   0.000241727       131.801      0.3729           1      127   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     114       4317.94   0.000113664       98.1512   3.657e-07       0.001      185  LS failed, Hessian reset \n",
      "     167       4318.45   0.000162663       159.036   1.933e-06       0.001      287  LS failed, Hessian reset \n",
      "     199       4318.53   1.93851e-06       67.7111      0.3429      0.3429      332   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     270        4318.6   5.26354e-06       78.7335   6.307e-08       0.001      499  LS failed, Hessian reset \n",
      "     299       4318.64   0.000275228       75.4504           1           1      529   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     367       4318.66   2.58033e-07       56.0546      0.7596      0.7596      617   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fbprophet.forecaster.Prophet at 0x7fc609d89d00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting one combination for training\n",
    "store_item_data = modeling_groups.get_group((2, 15)).drop(columns=['store', 'item'])\n",
    "\n",
    "# train-test split\n",
    "train_data = store_item_data.iloc[:-91, :]\n",
    "test_data = store_item_data.iloc[-91:, :]\n",
    "\n",
    "# creating a prophet model\n",
    "model = Prophet(\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    ")\n",
    "\n",
    "# model fitting\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef48ae",
   "metadata": {},
   "source": [
    "Now let's make future predictions in order to get data to traing XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9e9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasting the future three months\n",
    "future = model.make_future_dataframe(periods=len(test_data), freq='D')\n",
    "forecast = model.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e77640d",
   "metadata": {},
   "source": [
    "Merging Prophet components with XGBoost train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9421ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>trend</th>\n",
       "      <th>daily</th>\n",
       "      <th>weekly</th>\n",
       "      <th>yearly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257466</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>65</td>\n",
       "      <td>89.013</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257467</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>71</td>\n",
       "      <td>89.048</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257468</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>53</td>\n",
       "      <td>89.084</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257469</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>68</td>\n",
       "      <td>89.119</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257470</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>77</td>\n",
       "      <td>89.154</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ds   y   trend  daily  weekly  yearly\n",
       "257466  2013-01-01  65  89.013 -0.009  -0.081  -0.276\n",
       "257467  2013-01-02  71  89.048 -0.009  -0.078  -0.276\n",
       "257468  2013-01-03  53  89.084 -0.009  -0.006  -0.277\n",
       "257469  2013-01-04  68  89.119 -0.009   0.052  -0.278\n",
       "257470  2013-01-05  77  89.154 -0.009   0.127  -0.279"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting prophet seasonal and trend components\n",
    "prophet_components = forecast.loc[:, ['trend', 'daily', 'weekly', 'yearly']]\n",
    "\n",
    "# merging prophet components as training data\n",
    "xgb_train_data = store_item_data.copy()\n",
    "xgb_train_data['trend'] = prophet_components.trend.values\n",
    "xgb_train_data['daily'] = prophet_components.daily.values\n",
    "xgb_train_data['weekly'] = prophet_components.weekly.values\n",
    "xgb_train_data['yearly'] = prophet_components.yearly.values\n",
    "\n",
    "xgb_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5054704",
   "metadata": {},
   "source": [
    "Let's do train-test split and build up XGBoost data matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b68e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "fbp_xgb_train_set = xgb_train_data.iloc[:-91, :]\n",
    "fbp_xgb_test_set = xgb_train_data.iloc[-91:, :]\n",
    "\n",
    "# predictors-target split\n",
    "x_train = fbp_xgb_train_set.drop(columns='y').iloc[:, 1:]\n",
    "x_test = fbp_xgb_test_set.drop(columns='y').iloc[:, 1:]\n",
    "\n",
    "y_train = fbp_xgb_train_set.y\n",
    "y_test = fbp_xgb_test_set.y\n",
    "\n",
    "# train-test XGBoost DMatrix\n",
    "train_dmatrix = xgb.DMatrix(\n",
    "    data=x_train,\n",
    "    label=y_train\n",
    ")\n",
    "test_dmatrix = xgb.DMatrix(\n",
    "    data=x_test,\n",
    "    label=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120811ec",
   "metadata": {},
   "source": [
    "Let's finally train our XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a38b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ty-rmse:107.65150\n",
      "[15]\ty-rmse:30.49649\n",
      "[30]\ty-rmse:13.49310\n",
      "[45]\ty-rmse:11.75148\n",
      "[60]\ty-rmse:11.41088\n",
      "[75]\ty-rmse:11.32943\n",
      "[90]\ty-rmse:11.32506\n",
      "[99]\ty-rmse:11.32215\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "xgb_parameters = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'colsample_bytree': 1,\n",
    "    'subsample': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'random_state': seed,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "model_xgb = xgb.train(\n",
    "    params=xgb_parameters,\n",
    "    dtrain=train_dmatrix,\n",
    "    num_boost_round=100,\n",
    "    evals=[(test_dmatrix, 'y')],\n",
    "    verbose_eval=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143b1d3",
   "metadata": {},
   "source": [
    "Let's consider a RMSE of 11, for a three-months prediction range a good fit, although it definitely could be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24223a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
